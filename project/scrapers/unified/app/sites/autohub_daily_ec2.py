import asyncio
import re
import csv
import os
import boto3
from datetime import datetime, timedelta, timezone
from playwright.async_api import (
    async_playwright,
    Error as PlaywrightError,
    TimeoutError as PlaywrightTimeoutError,
)
import logging
from logging import handlers

# === ì„¤ì • ===
SITE = "autohub"
BUCKET = os.environ.get("BUCKET", "whatlunch-s3")
LOG_PREFIX = os.environ.get("LOG_PREFIX", f"logs/{SITE}")
LOG_DIR = f"/app/logs/{SITE}"
os.makedirs(LOG_DIR, exist_ok=True)

# KST(UTC+9)
KST = timezone(timedelta(hours=9))
# yesterday_dt = datetime.now(KST) - timedelta(days=1)
# today_mode: í•„ìš” ì‹œ ì˜¤ëŠ˜ë¡œ í…ŒìŠ¤íŠ¸
# yesterday_dt = datetime.now(KST)
yesterday_dt = datetime.now(KST) - timedelta(days=1)
yesterday_folder = yesterday_dt.strftime("%Y-%m-%d")
yesterday_file = yesterday_dt.strftime("%Y%m%d")

LOG_FILE = os.path.join(LOG_DIR, f"crawl_{yesterday_file}.log")

# --- ë¡œê¹… ì„¤ì •: ì½˜ì†” + íŒŒì¼ ---
logger = logging.getLogger()
logger.setLevel(logging.INFO)
# ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±° í›„ ì¬ì„¤ì •
for h in list(logger.handlers):
    logger.removeHandler(h)

fmt = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
sh = logging.StreamHandler()
sh.setFormatter(fmt)
logger.addHandler(sh)

fh = handlers.RotatingFileHandler(
    LOG_FILE, maxBytes=10 * 1024 * 1024, backupCount=3, encoding="utf-8"
)
fh.setFormatter(fmt)
logger.addHandler(fh)


async def extract_data_from_page(page, yesterday_str):
    yesterdays_data = []
    should_stop_globally = False
    error_occurred = False
    try:
        rows = await page.locator(
            'tbody.text-center.text_vert_midd tr[role="row"]'
        ).all()
        if not rows:
            logging.warning("í˜„ì¬ í˜ì´ì§€ì—ì„œ ë°ì´í„° í–‰ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return [], False, False
        for row in rows:
            cols = await row.locator("td").all()
            if len(cols) != 7:
                continue
            auction_date = await cols[0].inner_text()
            if auction_date != yesterday_str:
                logging.info(
                    f"ì–´ì œ({yesterday_str})ì™€ ë‹¤ë¥¸ ë‚ ì§œ({auction_date})ì˜ ë°ì´í„°ë¥¼ ë°œê²¬í•˜ì—¬ ìˆ˜ì§‘ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤."
                )
                should_stop_globally = True
                break
            car_details_html = await cols[2].inner_html()
            model_match = re.search(r"<strong>(.*?)</strong>", car_details_html)
            full_model_name = model_match.group(1).strip() if model_match else ""
            model_parts = full_model_name.split(" ", 1)
            brand = model_parts[0]
            model_info = model_parts[1] if len(model_parts) > 1 else ""
            import re as _re

            br_match = _re.search(r"<br>(.*)", car_details_html, _re.DOTALL)
            other_details_text = br_match.group(1).strip() if br_match else ""
            other_details_text = _re.sub(r"<.*?>", "", other_details_text)
            other_details_text = _re.sub(r"\s{2,}", " ", other_details_text)
            other_details_parts = [
                part.strip() for part in other_details_text.split("|")
            ]
            while len(other_details_parts) < 5:
                other_details_parts.append("")
            price_text = await cols[6].locator("strong").inner_text()
            car_info = {
                "ê²½ë§¤ì¼": auction_date,
                "ê²½ë§¤ì¥": await cols[1].inner_text(),
                "ë¸Œëœë“œ": brand,
                "ì°¨ëŸ‰ì •ë³´": model_info,
                "ì—°ì‹": other_details_parts[0],
                "ë³€ì†ê¸°": other_details_parts[1],
                "ì—°ë£Œ": other_details_parts[2],
                "ë°°ê¸°ëŸ‰": other_details_parts[3],
                "ìƒ‰ìƒ": other_details_parts[4],
                "ìš©ë„": await cols[3].inner_text(),
                "ì£¼í–‰ê±°ë¦¬": await cols[4].inner_text(),
                "í‰ê°€": await cols[5].inner_text(),
                "ë‚™ì°°ê°€(ë§Œì›)": price_text.replace(",", ""),
            }
            yesterdays_data.append(car_info)
    except PlaywrightError as e:
        if "Execution context was destroyed" in str(e):
            logging.warning("í˜ì´ì§€ ì´ë™ ì¤‘ ì»¨í…ìŠ¤íŠ¸ê°€ íŒŒê´´ë˜ì–´ ì¬ì‹œë„í•©ë‹ˆë‹¤.")
            error_occurred = True
        else:
            raise e
    return yesterdays_data, should_stop_globally, error_occurred


async def main():
    """Autohub í¬ë¡¤ë§ + CSV ì—…ë¡œë“œ + ë¡œê·¸ ì—…ë¡œë“œ"""
    all_car_data = []
    browser = None
    local_file_name = None
    try:
        yesterday_str_for_compare = yesterday_folder
        logging.info(
            f"ğŸ” ì–´ì œ ë‚ ì§œ({yesterday_str_for_compare})ì˜ Autohub ê²½ë§¤ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."
        )
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True, args=["--no-sandbox"])
            page = await browser.new_page()
            logging.info("ğŸš€ í˜ì´ì§€ë¡œ ì´ë™í•©ë‹ˆë‹¤: https://www.sellcarauction.co.kr/...")
            await page.goto(
                "https://www.sellcarauction.co.kr/newfront/successfulbid/sb/front_successfulbid_sb_list.do",
                timeout=60000,
            )
            logging.info("âœ”ï¸ 'ê²€ìƒ‰' ë²„íŠ¼ì„ í´ë¦­í•©ë‹ˆë‹¤.")
            await page.locator("a.button.btn_small.btn_search").click()
            await page.wait_for_selector(
                'tbody.text-center.text_vert_midd tr[role="row"]',
                state="attached",
                timeout=30000,
            )
            page_num = 1
            stop_crawling = False
            MAX_RETRIES = 3
            while not stop_crawling:
                logging.info(f"--- í˜ì´ì§€ {page_num} ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ ---")
                success_on_page = False
                for attempt in range(MAX_RETRIES):
                    try:
                        await page.wait_for_selector(
                            'tbody.text-center.text_vert_midd tr[role="row"]',
                            state="attached",
                            timeout=10000,
                        )
                    except PlaywrightTimeoutError:
                        logging.warning(
                            f"âš ï¸ í˜ì´ì§€ {page_num} ë¡œë”© ì‹œê°„ ì´ˆê³¼. {attempt + 1}/{MAX_RETRIES}ë²ˆì§¸ ì¬ì‹œë„..."
                        )
                        await page.reload()
                        continue
                    current_page_data, stop_crawling, error_occurred = (
                        await extract_data_from_page(page, yesterday_str_for_compare)
                    )
                    if error_occurred:
                        logging.warning(
                            f"âš ï¸ í˜ì´ì§€ {page_num}ì—ì„œ ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜. {attempt + 1}/{MAX_RETRIES}ë²ˆì§¸ ì¬ì‹œë„..."
                        )
                        await page.reload()
                        continue
                    success_on_page = True
                    break
                if not success_on_page:
                    logging.error(
                        f"âŒ í˜ì´ì§€ {page_num} ìˆ˜ì§‘ì— {MAX_RETRIES}ë²ˆ ì‹¤íŒ¨í•˜ì—¬ ì¤‘ë‹¨"
                    )
                    stop_crawling = True
                if success_on_page and current_page_data:
                    all_car_data.extend(current_page_data)
                    logging.info(
                        f"âœ”ï¸ {len(current_page_data)}ê°œ ì°¨ëŸ‰ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ. (ì´ {len(all_car_data)}ê°œ)"
                    )
                elif success_on_page and not stop_crawling:
                    logging.warning(
                        "âš ï¸ í˜„ì¬ í˜ì´ì§€ì—ì„œ ì–´ì œ ë‚ ì§œ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
                    )
                if stop_crawling:
                    break
                try:
                    active_page_element = page.locator("ul.pagination li.active a")
                    if not await active_page_element.is_visible(timeout=5000):
                        logging.info("â­ í˜ì´ì§€ë„¤ì´ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ì¢…ë£Œ")
                        break
                    current_page_num_text = await active_page_element.inner_text()
                    current_page_num = int(current_page_num_text)
                    next_page_num = current_page_num + 1
                    next_page_button = page.locator(
                        f"//ul[contains(@class, 'pagination')]//a[text()='{next_page_num}']"
                    )
                    if await next_page_button.is_visible():
                        logging.info(f"âœ”ï¸ {next_page_num} í˜ì´ì§€ë¡œ ì´ë™")
                        await next_page_button.click()
                    else:
                        next_block_button = page.locator(
                            "//ul[contains(@class, 'pagination')]//a[text()='>']"
                        )
                        if await next_block_button.is_visible():
                            logging.info("âœ”ï¸ ë‹¤ìŒ í˜ì´ì§€ ë¸”ë¡ìœ¼ë¡œ ì´ë™")
                            await next_block_button.click()
                        else:
                            logging.info("â­ ë§ˆì§€ë§‰ í˜ì´ì§€ ë„ë‹¬")
                            break
                    await page.wait_for_selector(
                        'tbody.text-center.text_vert_midd tr[role="row"]',
                        state="attached",
                        timeout=30000,
                    )
                    page_num += 1
                except (PlaywrightError, ValueError) as e:
                    logging.error(f"âŒ í˜ì´ì§€ ì´ë™/ë²ˆí˜¸ í™•ì¸ ì˜¤ë¥˜: {e}")
                    break
    except Exception as e:
        logging.critical(f"ğŸš¨ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜: {e}", exc_info=True)
    finally:
        if browser:
            await browser.close()
            logging.info("âœ”ï¸ ë¸Œë¼ìš°ì € ì¢…ë£Œ")
        if not all_car_data:
            logging.warning("âŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ì–´ íŒŒì¼ ì €ì¥ ìƒëµ")
        else:
            logging.info(f"ğŸ’¾ ì´ {len(all_car_data)}ê°œ ì €ì¥ ë° ì—…ë¡œë“œ")
            try:
                local_file_name = f"{SITE}-{yesterday_file}-raw.csv"
                fieldnames = all_car_data[0].keys()
                with open(
                    local_file_name, "w", encoding="utf-8-sig", newline=""
                ) as csvfile:
                    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                    writer.writeheader()
                    writer.writerows(all_car_data)
                logging.info(f"âœ”ï¸ ë¡œì»¬ ì €ì¥: {local_file_name}")
                s3_key = (
                    f"raw/{SITE}/{yesterday_folder}/{SITE}-{yesterday_file}-raw.csv"
                )
                logging.info(f"S3 ì—…ë¡œë“œ ì‹œì‘: s3://{BUCKET}/{s3_key}")
                boto3.client("s3").upload_file(local_file_name, BUCKET, s3_key)
                logging.info(f"âœ”ï¸ ì—…ë¡œë“œ ì™„ë£Œ: s3://{BUCKET}/{s3_key}")
            except Exception as e:
                logging.error(f"âŒ CSV ì €ì¥/ì—…ë¡œë“œ ì˜¤ë¥˜: {e}", exc_info=True)
            finally:
                if local_file_name and os.path.exists(local_file_name):
                    os.remove(local_file_name)
                    logging.info(f"âœ”ï¸ ì„ì‹œ íŒŒì¼ ì‚­ì œ: {local_file_name}")
        # --- ë¡œê·¸ ì—…ë¡œë“œ ---
        try:
            log_s3_key = f"{LOG_PREFIX}/{yesterday_folder}/crawl_{yesterday_file}.log"
            boto3.client("s3").upload_file(LOG_FILE, BUCKET, log_s3_key)
            logging.info(f"âœ”ï¸ ë¡œê·¸ ì—…ë¡œë“œ ì™„ë£Œ: s3://{BUCKET}/{log_s3_key}")
        except Exception as e:
            logging.error(f"ë¡œê·¸ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")


if __name__ == "__main__":
    asyncio.run(main())
