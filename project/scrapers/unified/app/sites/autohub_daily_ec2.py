import asyncio
import re
import csv
import io
import os
import boto3
import logging
from logging import handlers
from datetime import datetime, timedelta, timezone
from playwright.async_api import (
    async_playwright,
    Error as PlaywrightError,
    TimeoutError as PlaywrightTimeoutError,
)

# === ê¸°ë³¸ ì„¤ì • ===
SITE = "autohub"
BUCKET = os.environ.get("BUCKET", "whatlunch-s3")  # ê¸°ë³¸ ë²„í‚· ì´ë¦„, í•„ìš”ì‹œ ë³€ê²½
LOG_PREFIX = os.environ.get("LOG_PREFIX", f"logs/{SITE}")
LOG_DIR = f"/app/logs/{SITE}"
os.makedirs(LOG_DIR, exist_ok=True)

# --- ì‹œê°„ëŒ€ ë° ë‚ ì§œ ì„¤ì • ---
KST = timezone(timedelta(hours=9))
yesterday_dt = datetime.now(KST) - timedelta(days=1)
yesterday_folder = yesterday_dt.strftime("%Y-%m-%d")
yesterday_file = yesterday_dt.strftime("%Y%m%d")

LOG_FILE = os.path.join(LOG_DIR, f"crawl_{yesterday_file}.log")

# --- ë¡œê¹… ì„¤ì • (ì½˜ì†” + íŒŒì¼) ---
logger = logging.getLogger()
logger.setLevel(logging.INFO)
# ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±°
for h in list(logger.handlers):
    logger.removeHandler(h)

fmt = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")

# ì½˜ì†” í•¸ë“¤ëŸ¬
sh = logging.StreamHandler()
sh.setFormatter(fmt)
logger.addHandler(sh)

# íŒŒì¼ í•¸ë“¤ëŸ¬
fh = handlers.RotatingFileHandler(
    LOG_FILE, maxBytes=10 * 1024 * 1024, backupCount=3, encoding="utf-8"
)
fh.setFormatter(fmt)
logger.addHandler(fh)


async def extract_data_from_page(page, yesterday_str):
    """
    í˜„ì¬ í˜ì´ì§€ì—ì„œ ì–´ì œ ë‚ ì§œì˜ ìë™ì°¨ ë°ì´í„°ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ë‹¤ë¥¸ ë‚ ì§œê°€ ë‚˜ì˜¤ê±°ë‚˜ í˜ì´ì§€ ì´ë™ ì˜¤ë¥˜ ë°œìƒ ì‹œ ì‹ í˜¸ë¥¼ ë³´ëƒ…ë‹ˆë‹¤.
    """
    yesterdays_data = []
    should_stop_globally = False
    error_occurred = False

    try:
        rows = await page.locator(
            'tbody.text-center.text_vert_midd tr[role="row"]'
        ).all()

        for row in rows:
            cols = await row.locator("td").all()
            if len(cols) != 7:
                continue

            auction_date = await cols[0].inner_text()

            if auction_date != yesterday_str:
                should_stop_globally = True
                break

            car_details_html = await cols[2].inner_html()
            model_match = re.search(r"<strong>(.*?)</strong>", car_details_html)
            full_model_name = model_match.group(1).strip() if model_match else ""

            model_parts = full_model_name.split(" ", 1)
            brand = model_parts[0]
            model_info = model_parts[1] if len(model_parts) > 1 else ""

            br_match = re.search(r"<br>(.*)", car_details_html, re.DOTALL)
            other_details_text = br_match.group(1).strip() if br_match else ""
            other_details_text = re.sub(r"<.*?>", "", other_details_text)
            other_details_text = re.sub(r"\s{2,}", " ", other_details_text)
            other_details_parts = [
                part.strip() for part in other_details_text.split("|")
            ]

            while len(other_details_parts) < 5:
                other_details_parts.append("")

            price_text = await cols[6].locator("strong").inner_text()

            car_info = {
                "ê²½ë§¤ì¼": auction_date,
                "ê²½ë§¤ì¥": await cols[1].inner_text(),
                "ë¸Œëœë“œ": brand,
                "ì°¨ëŸ‰ì •ë³´": model_info,
                "ì—°ì‹": other_details_parts[0],
                "ë³€ì†ê¸°": other_details_parts[1],
                "ì—°ë£Œ": other_details_parts[2],
                "ë°°ê¸°ëŸ‰": other_details_parts[3],
                "ìƒ‰ìƒ": other_details_parts[4],
                "ìš©ë„": await cols[3].inner_text(),
                "ì£¼í–‰ê±°ë¦¬": await cols[4].inner_text(),
                "í‰ê°€": await cols[5].inner_text(),
                "ë‚™ì°°ê°€(ë§Œì›)": price_text.replace(",", ""),
            }
            yesterdays_data.append(car_info)

    except PlaywrightError as e:
        if "Execution context was destroyed" in str(e):
            error_occurred = True
        else:
            logger.error(f"Playwright ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise e

    return yesterdays_data, should_stop_globally, error_occurred


async def save_data_to_s3(data, target_date):
    """
    ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ S3ì— CSV íŒŒì¼ë¡œ ì—…ë¡œë“œí•©ë‹ˆë‹¤.
    """
    if not data:
        logger.warning("S3ì— ì—…ë¡œë“œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    folder_date = target_date.strftime("%Y-%m-%d")
    file_date = target_date.strftime("%Y%m%d")
    s3_key = f"raw/{SITE}/{folder_date}/{SITE}-{file_date}-raw.csv"

    logger.info(
        f"ìˆ˜ì§‘ëœ {len(data)}ê°œ ë°ì´í„°ë¥¼ s3://{BUCKET}/{s3_key} ê²½ë¡œì— ì—…ë¡œë“œí•©ë‹ˆë‹¤."
    )

    csv_buffer = io.StringIO()
    # ë°ì´í„°ì˜ ì²« ë²ˆì§¸ í•­ëª©ì„ ê¸°ë°˜ìœ¼ë¡œ í•„ë“œ ì´ë¦„ ë™ì  ìƒì„±
    fieldnames = list(data[0].keys()) if data else []

    writer = csv.DictWriter(csv_buffer, fieldnames=fieldnames)
    writer.writeheader()
    writer.writerows(data)

    s3 = boto3.client("s3")
    try:
        s3.put_object(
            Bucket=BUCKET,
            Key=s3_key,
            Body=csv_buffer.getvalue().encode("utf-8-sig"),
        )
        logger.info("âœ”ï¸ S3 ì—…ë¡œë“œ ì™„ë£Œ!")
    except Exception as e:
        logger.error(f"âŒ S3 ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


async def main():
    """
    Playwrightë¥¼ ì‚¬ìš©í•˜ì—¬ ì›¹ ìŠ¤í¬ë˜í•‘ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ S3ì— ì €ì¥í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜
    """
    all_car_data = []
    browser = None

    try:
        yesterday_str_for_compare = yesterday_dt.strftime("%Y-%m-%d")
        logger.info(
            f"ğŸ” ì–´ì œ ë‚ ì§œ({yesterday_str_for_compare})ì˜ ê²½ë§¤ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."
        )

        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()

            logger.info("ğŸš€ í˜ì´ì§€ë¡œ ì´ë™í•©ë‹ˆë‹¤: https://www.sellcarauction.co.kr/...")
            await page.goto(
                "https://www.sellcarauction.co.kr/newfront/successfulbid/sb/front_successfulbid_sb_list.do",
                timeout=60000,
            )

            logger.info("âœ”ï¸ 'ê²€ìƒ‰' ë²„íŠ¼ì„ í´ë¦­í•©ë‹ˆë‹¤.")
            await page.locator("a.button.btn_small.btn_search").click()

            await page.wait_for_selector(
                'tbody.text-center.text_vert_midd tr[role="row"]',
                state="attached",
                timeout=30000,
            )

            page_num = 1
            stop_crawling = False
            MAX_RETRIES = 3

            while not stop_crawling:
                logger.info(f"--- í˜ì´ì§€ {page_num} ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ ---")

                success_on_page = False
                for attempt in range(MAX_RETRIES):
                    try:
                        await page.wait_for_selector(
                            'tbody.text-center.text_vert_midd tr[role="row"]',
                            state="attached",
                            timeout=10000,
                        )
                    except PlaywrightTimeoutError:
                        logger.warning(
                            f"âš ï¸ í˜ì´ì§€ {page_num} ë¡œë”© ì‹œê°„ ì´ˆê³¼. {attempt + 1}/{MAX_RETRIES}ë²ˆì§¸ ì¬ì‹œë„..."
                        )
                        await page.reload()
                        continue

                    current_page_data, stop_crawling, error_occurred = (
                        await extract_data_from_page(page, yesterday_str_for_compare)
                    )

                    if error_occurred:
                        logger.warning(
                            f"âš ï¸ í˜ì´ì§€ {page_num}ì—ì„œ ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜ ë°œìƒ. {attempt + 1}/{MAX_RETRIES}ë²ˆì§¸ ì¬ì‹œë„..."
                        )
                        await page.reload()
                        continue

                    success_on_page = True
                    break

                if not success_on_page:
                    logger.error(
                        f"âŒ í˜ì´ì§€ {page_num} ë°ì´í„° ìˆ˜ì§‘ì— {MAX_RETRIES}ë²ˆ ì‹¤íŒ¨í•˜ì—¬ í¬ë¡¤ë§ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤."
                    )
                    stop_crawling = True

                if success_on_page and current_page_data:
                    all_car_data.extend(current_page_data)
                    logger.info(
                        f"âœ”ï¸ {len(current_page_data)}ê°œ ì°¨ëŸ‰ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ. (ì´ {len(all_car_data)}ê°œ)"
                    )
                elif success_on_page and not stop_crawling:
                    logger.info(
                        "âœ”ï¸ í˜„ì¬ í˜ì´ì§€ì—ì„œ ì–´ì œ ë‚ ì§œì˜ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
                    )

                if stop_crawling:
                    break

                # --- í˜ì´ì§€ë„¤ì´ì…˜ ì²˜ë¦¬ ---
                try:
                    active_page_element = page.locator("ul.pagination li.active a")
                    if not await active_page_element.is_visible(timeout=5000):
                        logger.info(
                            "â­ í˜ì´ì§€ë„¤ì´ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ í¬ë¡¤ë§ì„ ì¢…ë£Œí•©ë‹ˆë‹¤."
                        )
                        break

                    current_page_num_text = await active_page_element.inner_text()
                    current_page_num = int(current_page_num_text)
                    next_page_num = current_page_num + 1

                    next_page_button = page.locator(
                        f"//ul[contains(@class, 'pagination')]//a[text()='{next_page_num}']"
                    )

                    if await next_page_button.is_visible():
                        logger.info(f"âœ”ï¸ {next_page_num} í˜ì´ì§€ë¡œ ì´ë™í•©ë‹ˆë‹¤.")
                        await next_page_button.click()
                    else:
                        next_block_button = page.locator(
                            "//ul[contains(@class, 'pagination')]//a[text()='>']"
                        )
                        if await next_block_button.is_visible():
                            logger.info("âœ”ï¸ ë‹¤ìŒ í˜ì´ì§€ ë¸”ë¡ìœ¼ë¡œ ì´ë™í•©ë‹ˆë‹¤.")
                            await next_block_button.click()
                        else:
                            logger.info(
                                "â­ ë§ˆì§€ë§‰ í˜ì´ì§€ì— ë„ë‹¬í•˜ì—¬ í¬ë¡¤ë§ì„ ì¢…ë£Œí•©ë‹ˆë‹¤."
                            )
                            break

                    await page.wait_for_selector(
                        'tbody.text-center.text_vert_midd tr[role="row"]',
                        state="attached",
                        timeout=30000,
                    )
                    page_num += 1
                except (PlaywrightError, ValueError) as e:
                    logger.error(f"âŒ í˜ì´ì§€ ì´ë™ ë˜ëŠ” ë²ˆí˜¸ í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    break

    except Exception as e:
        logger.critical(f"\nğŸš¨ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}", exc_info=True)

    finally:
        if browser:
            await browser.close()
            logger.info("\nâœ”ï¸ ë¸Œë¼ìš°ì €ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

        # ë°ì´í„° S3ì— ì €ì¥
        await save_data_to_s3(all_car_data, yesterday_dt)

    # --- ë¡œê·¸ ì—…ë¡œë“œ ---
    try:
        log_s3_key = f"{LOG_PREFIX}/{yesterday_folder}/crawl_{yesterday_file}.log"
        boto3.client("s3").upload_file(LOG_FILE, BUCKET, log_s3_key)
        logger.info(f"âœ”ï¸ ë¡œê·¸ ì—…ë¡œë“œ ì™„ë£Œ: s3://{BUCKET}/{log_s3_key}")
    except Exception as e:
        logger.error(f"âŒ ë¡œê·¸ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")


if __name__ == "__main__":
    asyncio.run(main())
